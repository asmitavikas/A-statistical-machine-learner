Q1. What is the difference between filter, wrapper, and embedded methods for feature selection?
A1. Wrapper : defines usefulness of features based on classifier performance. 
	Filter : pick up intrinsic properties of the features measured via univariate statistics instead of cross-validation performance.
	Embedded : Similar to wrapper method, but used during learning, not after learning
	Filter methods:
		information gain
		chi-square test
		fisher score
		correlation coefficient
		variance threshold
	Wrapper methods:
		recursive feature elimination
		sequential feature selection algorithms
		genetic algorithms
	Embedded methods:
		L1 (LASSO) regularization
		decision tree

Q2. Collaborative filtering for movie ratings?
A2. dot product of embedding vectors + movie bias + user bias

Q3. Types of algorithm for recommendation?
A3. content filtering and collaborative filtering
	content filtering : uses known information about the products(price, description) and users (demographics, questionarie info) to make recommendations. 
	collaborative filtering: uses users input/behavior history to make recommendations

Q4. What is ALS?
A4. ALS stands for alternating least squares. It is used in recommender systems where we use r = p(T)q (where p = user rating, q = movie rating)
	In stochastic gradient descent, we compute the error  (r - P(T)q) and then update the parameters by a factor in opposite direction of the gradient 
	However, in ALS, we fix either p or q to convert a non-convex optimization problem to a quadratic. ALS fixes each one of those alternatively. When one is fixed, the other one is computed, and vice versa.
	ALS is easy to parallelize and can be used for non-sparse datasets easily.
	Hence ALS has implementation using spark. (https://www.quora.com/What-is-the-Alternating-Least-Squares-method-in-recommendation-systems-And-why-does-this-algorithm-work-intuition-behind-this)

Q5. How to do null imputation? How would it work on a sparse dataset?
A5. Depends on the type of data. Lets say we have a movie recommendation system. If data is missing there, the user has not seen the movie. Good idea to put 0 there (implicit feedback)
	Else put some random values like -99999
	Use KNN to find nearest neighbours, and use there values

Q6. What is negative sampling and implicit feedback in recommendation?
A6. Let us take Netflix as an example. Let us say we have a dataset of 100 movies with 10 movies in each genre.
	A user X watches only Comedy and Action movies. (2 genres = 20 movies). X has watched 6 comedy movies and 5 action movies.
	Negative sampling : When trying to recommend new movies, use only 20 movies as the new dataset for user X.
	Implicit Feedback : In comedy and action genre, replace missing values with 0. The idea is that, if X has chosen "not to watch" those 4 comedy movies, the probabilty of X rating those missing 4 comedy movies as +ve is very low. So put 0


Q7. Does gradient descent always converge to the same point?
A7. No, they do not because in some cases it reaches a local minima or a local optima point. You don’t reach the global optima point. It depends on the data and starting conditions.

Q8. What is the goal of A/B Testing?
A8. In web analytics, A/B testing (bucket tests or split-run testing) is a controlled experiment with two variants, A and B.

Q9. How to prevent overfitting?
A9. 1. collect more data
	2. use ensembling methods that “average” models
	3. choose simpler models / penalize complexity

Q10. Loss function for recommendation?
A10. Rating prediction : MSE
	 Whether a person will watch a movie next or not : Log loss

	 Metric : Accuracy

Q11. Ensemble, Bagging and Boosting?
A11. Boosting : is an ensemble technique in which the predictors are not made independently, but sequentially.
	 Bagging : ensembling technique in which we build many independent predictors/models/learners and combine them using some model averaging techniques.


Q12. What is ADABoost?
A12. All observations are given some initial weight. w_i = i/N
	 For each tree, 
	 	find weighted error. w_i*I(y_i != y_hat_i)
	 	find parameter of the tree (alpha)= log((1-err)/err)
	 	new weights of each data point = alpha * w_i (1 for prediction right, 0 for prediction wrong)

	 So each data point is weighted according to the number of times it has been identified wrong.
	 Final model : alpha_i for each model

Q13. Gradient Boost?
A13. At each tree, we want to minimize the MSE using Gradient Descent. 
	 For pth tree, let each observation be y_i
	 MSE = sum(y - y_i)
	 y_i = y_i - eta * d(MSE)/d(y_i)
	 	 = y_i - eta * 2 * sum(y - y_i)

Q14. Feature Engineering process?
A14. 1. Decompose Categorical Attributes - from color = ['red', 'green', 'unknown'] -> make 'is_red', 'is_green', 'is_unknown'
	 2. Decompose a Date-Time
	 3. Reframe Numerical Quantities - weight in grams 6508 can be made into new feature 6kg. Make new features like 'is_above_5kg'. 25% quantile of a value per group

Q15. How to compare two algorithm on the same data set?
A15. Method 1: 
		a. use K-fold cross validation to find the mean score (lets say F1 or something)
		b. we have the mean and variance of two competing algorithms
		c. use paired-t test to compare the two algorithms
	Method 2: Sign-test (classification context): 
		a. Count the number of times A performed better than B
		b. Assume it comes from a binomial distribution
		c. Ho : A = B then perform hypothesis testing
	Method 3: Wilcoxon signed rank test (classification context):
		a. the wins (A is better than B) are weighted and assumed coming from a symmetric distribution around a common median. Then, we obtain a p-value of the Ho test.

Q16. Typical modeling process
A16. a. Check for nulls, impute them
	 b. check for outliers in data (if a regression model) -> deal with them, either remove, or log transform etc
	 c. feature engineering
	 d. model creation
	 e. find evaluation metrics (depending upon class imbalance: roc curve, f score, log loss etc)
	 f. model evaluation (k fold etc) and parameter tuning
	 g. interpretation of results

Q17. How to add new features to existing model
A17. Evaluate if the feature is any good or not?
	 a. Box plot of the distribution of the classes vs the feature. If there is decent amout of seperation between the two classes, it means there is some signal in the noise.
	 b. Check to see if the new feature is somehow related with the old features or not
	 c. Look for old features and see if there is a way of combining them to form the new feature.
	 d. Use a tree model to check the feature importance.


Q18. Optmization function for different models.
A18. a. Binary Classifier : LogLoss, Precisio, F Score
     b. Multiclass classifier : confusion matrix of the different classes
     c. ROC Curve and AUC : Class imbalance
     d. R^2 : Regression

Q19. What are the drawbacks of using supervised machine learning to predict frauds?
A19. We have labels for our data in supervised learning. However in unsupervised, we cluster similar datapoints and try to identify them as fraud/non-fraud.
	 a. Cold start might be a problem in supervised learning. No fraud points to begin with, or new type of fraud data.
	 b. There might be 'blind connections' in the data, which supervised model overlooks. The model can overlook a seemingly obvious connection such as a shared card between two accounts. eg. we model fraud data on whether a person drives a car or not. If a person drives car 'A' -> fraud. another person does not drive 'A', but owns 'A'. Our model will ignore it

Q20. Bootstraping, bagging, stacking and ensembling?
A20. 1. Bootstrapping is the process of repeatedly drawing samples of equal size from a population. These samples are drawn with replacement.
	 
	 2. Bagging is bootstrap aggregation. It is an approach based on bootstrapping. We produce multiple training sets by sampling with replacement from the population. From each sample, we build a model. Then we ensemble multiple models together with equal weight. 

	 3. Boosting is an iterative process. We build a model on the dataset, then try to reduce the error iteratively by predicting them.

	 4. Stacking : We build several models on our original data (m1,..,m12). Then we use another model(logistic regression) to estimate the inputs together with the outputs and give weights to each model (m1,..,m12)
	 Each model m1,..,m12 gave us an output. Use those outputs as input and m1,..m12 as features in logistic regression, to determine importance of each feature. Use that importance as weights.

Q21. Common resampling methods?
A21. 1. Cross validation - with no replacement. Used to evaluate model performance and select models
	 2. Bootstrapping - with replacement. used to improve models.

Q22. What’s your favorite algorithm, and can you explain it to me in less than a minute?
A22. My favorite alogrithm would be Random Forest because it is generally not based on any assumptions about the distribution of data or errors (like Regression).
	 Random Forests are all about a set of rules and building our outputs on those set of rules. Then the majority output is taken as the final output.

Q23. What’s the difference between a generative and discriminative model?
A23. Let us say we have an input data x and a target label y.
	 A generative model learns the joint probability distribution p(x,y) and a discriminative model learns the conditional probability distribution p(y|x)
	 Discriminative models will generally outperform generative models on classification tasks.

Q24. What kind of cross-validation would you use for time-series data?
A24. Distribute data into train and test based on chronological order, rather than k-fold cross validation.
	 fold 1 : training [1], test [2]
	 fold 2 : training [1 2], test [3]
	 fold 3 : training [1 2 3], test [4]
	 fold 4 : training [1 2 3 4], test [5]
	 fold 5 : training [1 2 3 4 5], test [6]  

Q25. What is F1 score and how do we use it?
A25. Arithmatic mean of precision and recall
	 Used when True Negative does not matter much.

Q26. How would you handle an imbalanced dataset?
A26. 1. Collect more data
	 2. Replicate data to reduce imbalance and use a tree based model
	 3. Change score to ROC 

Q27. How to ensure overfitting is not happening?
A27. Keep the model simple
	 Cross validation
	 Regularization

Q28. Why is gradient boosting able to handle categorical variables?
A28. Because it needs to boost the base model, and it does not matter what does it uses to boost the model.

Q29. How does gradient boosting avoid overfitting?
A29. 1. Early stopping
	 2. Regularization -> restrict variables
	 3. min child weight -> like min sample leaf for RF (minimum number of samples allowed in a leaf)

Q30. Options for measuring feature importance in XGBoost?
A30. 1. Weight : number of times each feature was used to split across all trees
	 2. Cover : (number of times each feature was used to split across all trees) and (weighted by number of training observations that went through that split)
	 3. Gain : improvement in accuracy by each feature

Q31. What makes a measure of feature importance good or bad?
A31. Consistency : should give same results everytime its run
	 Accuracy : correct results

Q32. Gradient descend vs stochastic gradient decent?
A32. In gradient descend, we calculate the derivate for each data point and then sum it up. We do this till we reach the minima. This can be computationally very expensive. 
	 However, in stochastic gradient descend, we take only only 1 example each time. Calculate the gradient for that, and decend in that direction. Do till minima is reached. The example is chosen randomly each time (hence stochastic). The path is noisier, but it eventually reaches the same result and faster.