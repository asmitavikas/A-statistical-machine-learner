Q1. What is the difference between filter, wrapper, and embedded methods for feature selection?
A1. Wrapper : defines usefulness of features based on classifier performance. 
	Filter : pick up intrinsic properties of the features measured via univariate statistics instead of cross-validation performance.
	Embedded : Similar to wrapper method, but used during learning, not after learning
	Filter methods:
		information gain
		chi-square test
		fisher score
		correlation coefficient
		variance threshold
	Wrapper methods:
		recursive feature elimination
		sequential feature selection algorithms
		genetic algorithms
	Embedded methods:
		L1 (LASSO) regularization
		decision tree

Q2. Collaborative filtering for movie ratings?
A2. dot product of embedding vectors + movie bias + user bias

Q3. Types of algorithm for recommendation?
A3. content filtering and collaborative filtering
	content filtering : uses known information about the products(price, description) and users (demographics, questionarie info) to make recommendations. 
	collaborative filtering: uses users input/behavior history to make recommendations

Q4. What is ALS?
A4. ALS stands for alternating least squares. It is used in recommender systems where we use r = p(T)q (where p = user rating, q = movie rating)
	In stochastic gradient descent, we compute the error  (r - P(T)q) and then update the parameters by a factor in opposite direction of the gradient 
	However, in ALS, we fix either p or q to convert a non-convex optimization problem to a quadratic. ALS fixes each one of those alternatively. When one is fixed, the other one is computed, and vice versa.
	ALS is easy to parallelize and can be used for non-sparse datasets easily.
	Hence ALS has implementation using spark. (https://www.quora.com/What-is-the-Alternating-Least-Squares-method-in-recommendation-systems-And-why-does-this-algorithm-work-intuition-behind-this)

Q5. How to do null imputation? How would it work on a sparse dataset?
A5. Depends on the type of data. Lets say we have a movie recommendation system. If data is missing there, the user has not seen the movie. Good idea to put 0 there (implicit feedback)
	Else put some random values like -99999
	Use KNN to find nearest neighbours, and use there values

Q6. What is negative sampling and implicit feedback in recommendation?
A6. Let us take Netflix as an example. Let us say we have a dataset of 100 movies with 10 movies in each genre.
	A user X watches only Comedy and Action movies. (2 genres = 20 movies). X has watched 6 comedy movies and 5 action movies.
	Negative sampling : When trying to recommend new movies, use only 20 movies as the new dataset for user X.
	Implicit Feedback : In comedy and action genre, replace missing values with 0. The idea is that, if X has chosen "not to watch" those 4 comedy movies, the probabilty of X rating those missing 4 comedy movies as +ve is very low. So put 0


Q7. Does gradient descent always converge to the same point?
A7. No, they do not because in some cases it reaches a local minima or a local optima point. You don’t reach the global optima point. It depends on the data and starting conditions.

Q8. What is the goal of A/B Testing?
A8. In web analytics, A/B testing (bucket tests or split-run testing) is a controlled experiment with two variants, A and B.

Q9. How to prevent overfitting?
A9. 1. collect more data
	2. use ensembling methods that “average” models
	3. choose simpler models / penalize complexity

Q10. Loss function for recommendation?
A10. Rating prediction : MSE
	 Whether a person will watch a movie next or not : Log loss

	 Metric : Accuracy

Q11. Ensemble, Bagging and Boosting?
A11. Boosting : is an ensemble technique in which the predictors are not made independently, but sequentially.
	 Bagging : ensembling technique in which we build many independent predictors/models/learners and combine them using some model averaging techniques.


Q12. What is ADABoost?
A12. All observations are given some initial weight. w_i = i/N
	 For each tree, 
	 	find weighted error. w_i*I(y_i != y_hat_i)
	 	find parameter of the tree (alpha)= log((1-err)/err)
	 	new weights of each data point = alpha * w_i (1 for prediction right, 0 for prediction wrong)

	 So each data point is weighted according to the number of times it has been identified wrong.
	 Final model : alpha_i for each model

Q13. Gradient Boost?
A13. At each tree, we want to minimize the MSE using Gradient Descent. 
	 For pth tree, let each observation be y_i
	 MSE = sum(y - y_i)
	 y_i = y_i - eta * d(MSE)/d(y_i)
	 	 = y_i - eta * 2 * sum(y - y_i)

Q14. Feature Engineering process?
A14. 1. Decompose Categorical Attributes - from color = ['red', 'green', 'unknown'] -> make 'is_red', 'is_green', 'is_unknown'
	 2. Decompose a Date-Time
	 3. Reframe Numerical Quantities - weight in grams 6508 can be made into new feature 6kg. Make new features like 'is_above_5kg'. 25% quantile of a value per group

Q15. How to compare two algorithm on the same data set?
A15. Method 1: 
		a. use K-fold cross validation to find the mean score (lets say F1 or something)
		b. we have the mean and variance of two competing algorithms
		c. use paired-t test to compare the two algorithms
	Method 2: Sign-test (classification context): 
		a. Count the number of times A performed better than B
		b. Assume it comes from a binomial distribution
		c. Ho : A = B then perform hypothesis testing
	Method 3: Wilcoxon signed rank test (classification context):
		a. the wins (A is better than B) are weighted and assumed coming from a symmetric distribution around a common median. Then, we obtain a p-value of the Ho test.

Q16. Typical modeling process
A16. a. Check for nulls, impute them
	 b. check for outliers in data (if a regression model) -> deal with them, either remove, or log transform etc
	 c. feature engineering
	 d. model creation
	 e. find evaluation metrics (depending upon class imbalance: roc curve, f score, log loss etc)
	 f. model evaluation (k fold etc) and parameter tuning
	 g. interpretation of results

Q17. How to add new features to existing model
A17. Evaluate if the feature is any good or not?
	 a. Box plot of the distribution of the classes vs the feature. If there is decent amout of seperation between the two classes, it means there is some signal in the noise.
	 b. Check to see if the new feature is somehow related with the old features or not
	 c. Look for old features and see if there is a way of combining them to form the new feature.
	 d. Use a tree model to check the feature importance.


Q18. Optmization function for different models.
A18. a. Binary Classifier : LogLoss, Precisio, F Score
     b. Multiclass classifier : confusion matrix of the different classes
     c. ROC Curve and AUC : Class imbalance
     d. R^2 : Regression

Q19. What are the drawbacks of using supervised machine learning to predict frauds?
A19. We have labels for our data in supervised learning. However in unsupervised, we cluster similar datapoints and try to identify them as fraud/non-fraud.
	 a. Cold start might be a problem in supervised learning. No fraud points to begin with, or new type of fraud data.
	 b. There might be 'blind connections' in the data, which supervised model overlooks. The model can overlook a seemingly obvious connection such as a shared card between two accounts. eg. we model fraud data on whether a person drives a car or not. If a person drives car 'A' -> fraud. another person does not drive 'A', but owns 'A'. Our model will ignore it

Q20. Bootstraping, bagging, stacking and ensembling?
A20. 1. Bootstrapping is the process of repeatedly drawing samples of equal size from a population. These samples are drawn with replacement.
	 
	 2. Bagging is bootstrap aggregation. It is an approach based on bootstrapping. We produce multiple training sets by sampling with replacement from the population. From each sample, we build a model. Then we ensemble multiple models together with equal weight. 

	 3. Boosting is an iterative process. We build a model on the dataset, then try to reduce the error iteratively by predicting them.

	 4. Stacking : We build several models on our original data (m1,..,m12). Then we use another model(logistic regression) to estimate the inputs together with the outputs and give weights to each model (m1,..,m12)
	 Each model m1,..,m12 gave us an output. Use those outputs as input and m1,..m12 as features in logistic regression, to determine importance of each feature. Use that importance as weights.

Q21. Common resampling methods?
A21. 1. Cross validation - with no replacement. Used to evaluate model performance and select models
	 2. Bootstrapping - with replacement. used to improve models.

Q22. What’s your favorite algorithm, and can you explain it to me in less than a minute?
A22. My favorite alogrithm would be Random Forest because it is generally not based on any assumptions about the distribution of data or errors (like Regression).
	 Random Forests are all about a set of rules and building our outputs on those set of rules. Then the majority output is taken as the final output.

Q23. What’s the difference between a generative and discriminative model?
A23. Let us say we have an input data x and a target label y.
	 A generative model learns the joint probability distribution p(x,y) and a discriminative model learns the conditional probability distribution p(y|x)
	 Discriminative models will generally outperform generative models on classification tasks.

Q24. What kind of cross-validation would you use for time-series data?
A24. Distribute data into train and test based on chronological order, rather than k-fold cross validation.
	 fold 1 : training [1], test [2]
	 fold 2 : training [1 2], test [3]
	 fold 3 : training [1 2 3], test [4]
	 fold 4 : training [1 2 3 4], test [5]
	 fold 5 : training [1 2 3 4 5], test [6]  

Q25. What is F1 score and how do we use it?
A25. Arithmatic mean of precision and recall
	 Used when True Negative does not matter much.

Q26. How would you handle an imbalanced dataset?
A26. 1. Collect more data
	 2. Replicate data to reduce imbalance and use a tree based model
	 3. Change score to ROC 

Q27. How to ensure overfitting is not happening?
A27. Keep the model simple
	 Cross validation
	 Regularization

Q28. Why is gradient boosting able to handle categorical variables?
A28. Because it needs to boost the base model, and it does not matter what does it uses to boost the model.

Q29. How does gradient boosting avoid overfitting?
A29. 1. Early stopping
	 2. Regularization -> restrict variables
	 3. min child weight -> like min sample leaf for RF (minimum number of samples allowed in a leaf)

Q30. Options for measuring feature importance in XGBoost?
A30. 1. Weight : number of times each feature was used to split across all trees
	 2. Cover : (number of times each feature was used to split across all trees) and (weighted by number of training observations that went through that split)
	 3. Gain : improvement in accuracy by each feature

Q31. What makes a measure of feature importance good or bad?
A31. Consistency : should give same results everytime its run
	 Accuracy : correct results

Q32. Gradient descend vs stochastic gradient decent?
A32. In gradient descend, we calculate the derivate for each data point and then sum it up. We do this till we reach the minima. This can be computationally very expensive. 
	 However, in stochastic gradient descend, we take only only 1 example each time. Calculate the gradient for that, and decend in that direction. Do till minima is reached. The example is chosen randomly each time (hence stochastic). The path is noisier, but it eventually reaches the same result and faster.

Q33. Why use Gradient Descent over Stochastic Gradient Descent and vice versa?
A33. GD theoritically minimizes the error function better than SGD (because we are looking at all observations). So GD is preferable for smaller datasets, where accuracy is the concern
	 Whereas SGD is faster and is preferable for larger datasets where we have a speed-accuracy tradeoff.

Q34. Can we overfit a model even after splitting into test and train?
A34. Yes, if we retune a model after checking the performance on test data, then we tend to overfit on the test set.

Q35. Advantages and disadvantages of decision trees?
A35. Decision trees are 
		easy to interpret 
		robust to outliers (as they are non parametric)
		less parameters to tune
	 Bad
	 	easily overfits - can be reduced by random forest

Q36. Advantages and disadvantages of neural netowrks?
A36. Good
		high precision
		high flexibiltiy in terms of learning new patterns
	Bad
		less interpretable - after you increase the layers
		needs large traning data
		tuning is difficult

Q37. How can you choose a classifier based on training set size?
A37. Small training sample : naive bayes behaves better - less overfitting
	 Large sample : Logistic Regression - accounts for complex relationships

Q38. Explain Latent Dirichlet Allocation (LDA)
A38. It is an common method of topic modeling
	 "Dirichlet" distribution is simply a distribution of distributions
	 Every document is a distribution of topics and every topic is a distribution of words

Q39. Why are ensembles better than individual models?
A39. Averages out bias
	 Reduces variance
	 less overfit

Q40. RF and boosting in terms of bias variance?
A40. In RF each decision tree gives us low bias. By increasing the number of trees, we reduce variance and hence reduce the chance of overfitting
	 In boosting each tree gives high bias but low variance (less overfitting). By increasing the number of boosted trees by increasing the number of times we perform boosting, we reduce the bias (error between true and predicted)

Q41. Dimensionality reduction on large volume of data (that can't be fit in memory)
A41. Randomly sample rows or do stratified random sampling, then load into memory
	 Load only numerical columns - do PCA
	 Load only categorical columns - do chi-squared test to find if two columns are related.
	 	Ho - not related
	 	Ha - related

Q42. After spending several hours, you are now anxious to build a high accuracy model. As a result, you build 5 GBM models, thinking a boosting algorithm would do the magic. Unfortunately, neither of models could perform better than benchmark score. Finally, you decided to combine those models. Though, ensembled models are known to return high accuracy, but you are unfortunate. Where did you miss?
A42. Ensembles combine weak learners to create strong learner. However, ensembles work best when the models itself are not correlated. 
	 If all 5 GBM models are not doing well, it could mean that the models itself are correlated. eg. Model 1 incorrectly classifies User 1 to Bucket 5. All other models also put User 1 to Bucket 5. So they are all correlated

Q43. Can removing intercept term in regression change R2 from 0.3 to 0.8?
A43. Yes. Intercept makes the model that is equal to mean (without any independent variable in picture).
	 When intercept is present, R2 evaluates the model wrt to the mean model
	 When no intercept, R2 makes no such assumptions ie, it removes the mean

Q44. When is Ridge regression favorable over Lasso regression?
A44. Ridge does not remove the variables, but makes them very close to 0. Ridge is used when 'prediction' is the output.
	 When we have few variables with large effect -> use lasso
	 When we have many variables with small effect -> use ridge

Q45. How does a tree split values?
A45. Based on gini index and entropy. Finds the best features that splits the data into purest form.
	 Gini index : if we select two items from a population, they should be of the same class. We find the probability of them being in the same class.
	 	Calculate the gini index of each of the subnode, using formula sum of square of probability for success and failure (p^2+q^2).
	 	Calculate Gini for split using weighted Gini score of each node of that split  

	 Entropy is the measure of impurity as given by (for binary class):
	 	- p log p - q log q
	 	probability of success and failure respectively in that node. 

	 Low entropy and high gini index is needed

Q46. How do you fit a non linear relation to a linear model?
A46. This can be done by fitting a linear model to nonlinear terms. For example, linear regression for a simple two variable case just fits a line y = mx + b which is a linear model between y and x. If instead of modeling vs x you squared all of the values of x and then repeated the procedure, you would be modeling y = mx^2 + b. The modeling process doesn't change, you are just applying it to nonlinear terms.

Q47. When to use Kfold vs LOOC (leave one out crossvalidation)?
A47. There is a bias variance tradeoff between the two.
	 LOOC gives low bias but high variance. Low bias because training set = n-1 examples, so it might cover test set well. High variance, coz any change in training set will drastically change the model.
	 K-fold : high bias but low variance. less overlap

	 Use LOOC when you have less data

Q48. How to identify if validation set is good?
A48. If increase in validation set error increases the test set error and decrease