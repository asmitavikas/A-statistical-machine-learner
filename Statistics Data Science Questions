Q1. When is it safe to assume "normal" distribution.
A1. CLT states that when the sample sizes are large enough, sample means tend to have normal distributions. N=30 is a valid lower bound on sample size. We can also build the model and then plot QQ plots or apply Shapiro-Wilk or Kolmogrov-Smirnov's test to find normality.

Q2. if confidence interval of two groups is [-10,-8] how would  you explain it? (Google)
A2. Assume confidence interval of two groups = confidence interval of difference in data between two groups
	Let us take the confidence level = 95%. 
	We are 95% confident that the mean of the difference between two groups lies between -10 and -8.
	Mean would be -9 and standard error around 1/2

Q3. Explain confidence interval to someone without a statistics background
A3. Confidence intervals capture how confident we are about a particular value. If we're not confident, the interval will be large. It's like we're saying, "I think this is the right value, but it could be as low as this or as high as this. We're not very confident in it". If the confidence interval is high, it's because we have a small sample size, a lot of variation, or both.

Q4. Bootstrap resampling?
A4. Bootstrapping is a statistical method that uses data resampling with replacement (see: generate_sample_indices) to estimate the robust properties of nearly any statistic.

Q5. What is p-value?
A5. P-value is essentially the probability of rejecting the null hypothesis. And how do we reject the null hypothesis? By observing samples beyond the scope of null hypothesis. Thus P-value is the probability of observing samples more extreme than our data, given null hypothesis is true.
	Small p-value = reject null hypothesis


Q6. What is the difference between Bayesian Estimate and Maximum Likelihood Estimation (MLE)?
A6. In bayesian estimate we have some knowledge about the data/problem (prior) .There may be several values of the parameters which explain data and hence we can look for multiple parameters like 5 gammas and 5 lambdas that do this. So we need with prior and posterior probabilties.
	Maximum likelihood does not take prior into consideration (ignores the prior) so it is like being a Bayesian  while using some kind of a flat prior.

Q7. What is an Eigenvalue and Eigenvector?
A7. In linear algebra, an eigenvector or characteristic vector of a linear transformation is a non-zero vector that only changes by a scalar factor, when that linear transformation is applied to it. 

Q8. What is the curse of dimensionality?
A8. This exponential growth in data causes high sparsity in the data set and unnecessarily increases storage space and processing time for the particular modelling algorithm. This makes clustering etc very difficult. 2d clusters can't be generalized to 7d

Q9. How can non-linear relations between X (age) and Y (income) fit into a linear model?
A9. https://www.quora.com/How-can-non-linear-relations-between-X-age-and-Y-income-fit-into-a-linear-model
	By approximating values between a range.

Q10. What is “naive” in a naive Bayes classifier?
A10. A naive Bayes classifier assumes that the presence (or absence) of a particular feature of a class is unrelated to the presence (or absence) of any other feature, given the class variable.
	Doesn't constitute for independency between features.
	eg. a fruit may be considered to be an apple if it is red, round, and about 4" in diameter. Even if these features depend on each other or upon the existence of the other features, a naive Bayes classifier considers all of these properties to independently contribute to the probability that this fruit is an apple.

Q11. What is bayesian vs frequentist view points?
A11. Bayesian says that data is fixed and parameters need to be defined
	 frequentist says that parameters are fixed and data need to be randomly sampled.

Q12. Difference between Bayesian estimator and Maximum Likelihood?
A12. Maximum likelihood finds parameters which maximizes the probability of the data point to appear
	 Bayesian calculates the prosterior probabilty of data point given a parameter

Q13. How to ensure random sampling for A/B tests?
A13. Use stratified sampling. Divide the population into homogenous groups, then randomly sample from those groups. Reduces variance.

Q14. How to find the influence of a person on twitter?
A14. http://thenoisychannel.com/2009/01/13/a-twitter-analog-to-pagerank

Q15. Explain prior probability, likelihood and marginal likelihood in context of naiveBayes algorithm?
A15. Prior probability is the proportion of dependent (binary) variable in the data set. 
		eg. (number of spam = yes)/ total data points

	 Likelihood is the probability of classifying a given observation as 1 in presence of some other variable.
	 	eg. The probability that the word ‘FREE’ is used in previous spam message is likelihood

	 Marginal likelihood the probability that the word ‘FREE’ is used in any message.

Q16. How does regularization reduces overfitting?
A16. By reducing the bias of the model and restricting β.
	 Overfitting is caused when the model is too complicated, thus it gets built in the shape of the train data.
	 If we introduce a regularization, it restricts the model by limiting the space that the parameter vector β can live in. β is the estimator here.
	 So model is simplier and hence overfitting is reduced

Q17. In regression, which loss function is best suited when we have outliers?
A17. MAE is good for outliers, but is non differentiable at 0
     MSE is bad for outliers, as being squared, it gives very high importance to errors and makes model complicated
     Huber loss is a good mix, MSE near 0 and MAE towards outliers

Q18. What is Bayesian neural network?
A18. Standard neural network is equivalent to looking at the likelihood of the function. We fix the parameters and try to find them. This is the frequentist approach.
	 But is it right? -> we ignore any uncertainty that we may have in the proper weight values, as we use point estimates.
	 In Bayesian neural network, we have a Neural Network with a prior distribution in its weights. Hence we get a 'distribution' of answer rather than a single answer.

	 This takes care of both -> regularization and model selection without cross-validation.

Q19. Give an example of a case in which you run an A/B test, test wins with significant p-value, but you still choose to not make the change?
A19. Sample size too low. bigger the sample size more is the statistical significance of a computed figure


Q20. What does statistical significance mean?
A20. Statistical significance means that there is less chance of sampling error affecting the mean differences. It comes from data and the confidence intervals.

	 Statistical significance means that there is a probability that a relationship between two variables exists.
	 A practical significance means that there exists a relationship between two variables in the real world.

Q21. Steps for A/B Testing?
A21. a. Find the business goal : to improve the website
	 b. Find the KPI used to measure this goal : what to measure, click through rate
	 c. Get the metrics from the KPI
	 d. Select the elements to test : two versions of the submit button
	 e. Design the test : mean/proportion, sample size, significance level
	 f. Divide population : stratified random etc
	 g. Let the test accumulate data : don't stop in between if significance level is reached
	 h. Document the results and implement the changes if needed

Q22. Type 1 vs Type 2 errors?
A22. Type 1 error is when we reject the null hypothesis, but the null hypothesis is true
	 Type 2 error is when we do not reject the null hypothesis, but the null hypothesis is false.

Q23. What is Fourier Transform?
A23. Given a generic function, it decomposes the generic function into a superimposition of symmetric functions.

Q24. What is multi-colinearity and how to detect and fix it?
A24. Multi-colinearity : two or more highly correlated variables
	 Multi-colinearity is bad because X(t)X is not invertible (in case of perfect mc and rank is <n)
	 Otherwise, the variance of the model is high, because change in one variable drastically changes one or more variables. 
	 Multi-colinearity thus leads to overfitting

	 We can detect multicolinearity using the VIF(variance inflated factor) - how much variance is in the model

	 Remove multicolinearity by
	 	removing one of correlated variables
	 	PCA
	 	Ridge regression

Q25. Why use timeseries when we have regression?
A25. Because in time series, we forecast values, which is equal to extrapolating the data.
	 In trying to extrapolate, if we are using linear regression, we can only predict trend in one direction.
	 Also, in time series, future is based on past -> in regression we do not take this into account

Q26. Parametric vs non-parametric model?
A26. parametric models : y_i = b0 + b_i*x_i + e_i
     non parametric models : y_i = f(x_i)  +e_i

     parametric models are easy to interpret. and easier to make predictions
     non-parameteric models are more robust and doesn't bother too much about outliers.

Q27. What is bootstrap?
A27. Given a large dataset, we need to estimate a statistics from that sample dataset. But we have only one dataset.
	 Bootstrap randomly samples from that dataset and creates sub-datasets that we have seen. 
	 For these sub-datasets, we create statistics for each of the sub-samples. This gives us the distribution of the statistics.
	 Bootstrap is always performed with replacement. 
	 if there are 'n' samples in the dataset. We can perform 1000 bootstrap samplings
	 	for each iteration, take 'n' samples with replacement, and compute mean(or any other statistics)
	 we will have a distribution of 1000 means.
	 take mean of the distribution to calculate the bootstrap mean.

	 Even functions can be bootstrapped. eg. slope of a regression line.
	 Draw a distribution for slope of the regression lines.

Q28. What is local optimum vs global optimum?
A28. Local optimum is when a solution is ideal only when seen in the context of its neighbours.
     Whereas global optimum is when the solution is ideal across all data points

     Cost function is always decreasing till the local optimum is reached, but not necessarily after that.

     To avoid local optimum -> do multiple iterations. For example in k-means, run k-means multiple times and then take the solution with lowest cost

Q29. Precision, recall and ROC?
A29. Precision : out of the entries predicted 1, how many are actually 1. --> 1 - specificity
	 Recall : out of the entries actually 1, how many are predicted 1. --> sensitivity 
	 ROC = true positive vs false positive --> sensitivity vs specificity

Q30. Power of statistical test?
A30. Type 1 Error = Rejecting null hypothesis when it is true
	 Type 2 Error = Not rejecting null hypothesis when it is false = Beta
	 Power = (1-Beta)
	 	   = P(X >= x | mean = mean of alternate hypothesis ie x_bar)  
	 	   = P(Z >= (x - x_bar)/(std/sqrt(n)))
	 	   = 1 - P (Z < (x - x_bar)/(std/sqrt(n)))

	 https://onlinecourses.science.psu.edu/stat414/book/export/html/245

	 If we have the power to begin with, eg 90%, we can calculate sample size